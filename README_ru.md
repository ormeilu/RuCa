# RuCa
RuCa Benchmark (pronounced "roo-ka")  - Russian Tool Calling Benchmark for LLM

Чтобы запустить бенчмарк:

```bash
uv run ruca --runs 2 --concurrent 8 --json
```

## Содержание
1. [Введение](#введение)
2. [Запуск бенчмарка](#запуск-бенчмарка)
3. [Структура датасета](#структура-датасета)
4. [Сценарии тестирования](#сценарии-тестирования)
5. [Система метрик](#система-метрик)
5. [Лидерборд](#лидерборд)

## Введение

Ru-Ca Benchmark - бенчмарк для оценки способностей больших языковых моделей (LLM) к вызову инструментов (tool calling). Проект измеряет точность выбора инструментов, корректность передачи параметров и обработку различных типов запросов на русском языке.

## Запуск бенчмарка

### Установка

```bash
# Клонирование репозитория
git clone https://github.com/ormeilu/RuCa.git
cd RuCa

# Установка зависимостей
uv sync
```

### Настройка окружения

Создайте файл `.env` на основе `env.template`.

Заполните `.env` вашими API_KEY и BASE_URL.

Центральный скрипт (`main.py`) управляет прогоном бенчмарка для всех моделей, описанных в `config.yaml`, и последовательно запускает их с параметрами, заданными в конфиге.

### Запуск

```bash

# Запуск с кастомными параметрами
uv run main.py --runs 2 --concurrent 8 --json
```

**Параметры:**
- `--runs` — количество повторных прогонов датасета в модель для усреднения результатов
- `--concurrent` — количество запросов для асинхронной подачи в модель
- `--json` — вывод результатов в формате JSON

## Структура датасета

Датасет представляет собой JSON-файлы с набором тестовых запросов, каждый из которых содержит ожидаемые данные, которые модель вернет.

### Формат записи

```json
{
    "queries_basic": [
        {
            "id": "000000",
            "complexity": "easy",
            "category": "tool_basic",
            "type": "ordinary",
            "query": "Сколько сейчас времени в Токио",
            "expected_tool": "get_time",
            "expected_parameters": {
                "format": "24h"
            },
            "requires_clarification": false,
            "skills": ["Decision", "Tool selection", "Params", "Result"]
        }
    ]
}
```

### Описание полей

| Поле | Тип | Описание |
|------|-----|----------|
| `id` | string | Уникальный идентификатор запроса |
| `complexity` | enum | Сложность: `easy`, `medium`, `hard` |
| `category` | string | Для какого домена запрос (например, `tool_basic`, `tool_airlines` и т.д.) |
| `type` | string | Тип сценария для запроса: `ordinary`, `misprint`, `ambiguous` и т.д.  |
| `query` | string | Текст запроса пользователя |
| `expected_tool` | string | Ожидаемый инструмент для вызова |
| `expected_parameters` | object | Ожидаемые параметры вызова |
| `requires_clarification` | boolean | Требуется ли уточнение у пользователя |
| `skills` | array | Список проверяемых метрик |

---

## Сценарии тестирования

Бенчмарк включает 8 сценариев, проверяющих различные аспекты способности моделей к tool calling.

### 1. Явные запросы

**Что проверяет:** Способность модели выполнять прямые инструкции по вызову конкретного инструмента.

**Пример:**
```
"query": "Вызови airbnb_search для Москвы на двоих взрослых"
```

---

### 2. Неявные запросы

**Что проверяет:** Способность модели самостоятельно определить нужный инструмент из контекста запроса.

**Пример:**
```
"query": "Хочу забронировать билет из Москвы в Париж на 2025-06-10 эконом класс, меня зовут Иван Иванов"
```

---

### 3. Запросы с опечатками

**Что проверяет:** Устойчивость модели к ошибкам и способность понимать запрос, несмотря на опечатки.

**Пример:**
```
"query": "Ищу жилье в Санкт-Питербурге на троих с 15 по 20 декабрья"
```

---

### 4. Запросы с шумом

**Что проверяет:** Способность модели фильтровать нерелевантную информацию и фокусироваться на сути запроса.

**Пример:**
```
"query": "Добавь в корзину кофеварку DeLonghi (ID 3125), кстати, мой e-mail — ivanov88@gmail.com"
```

---

### 5. Адаптивные запросы

**Что проверяет:** Способность модели адаптироваться к изменениям в инструкциях пользователя.

**Пример:**
```
"query": "Переведи 100 долларов в евро... нет, лучше в японские иены"
```

---

### 6. Неоднозначные запросы

**Что проверяет:** Способность модели распознавать недостаток информации и запрашивать уточнения вместо угадывания.

**Пример:**
```
"query": "Какая погода там, где сейчас полдень?"
```

---

### 7. Запросы с последовательным вызовом

**Что проверяет:** Способность модели выполнять инструменты в строгом порядке, когда результат одного вызова требуется для следующего.

**Пример:**
```
"query": "Найди мне недорогие беспроводные наушники и сразу переведи их цену в доллары."
```

---

### 8. Запросы-проверка на обработку ошибок

**Что проверяет:** Способность модели корректно обрабатывать ситуации, когда запрашивается несуществующий инструмент.

**Характеристика:** Пользователь просит вызвать инструмент, которого нет в списке доступных.

**Пример:**
```
"query": "Используй ChekInn_Toooor чтобы сделать мне регистрацию на рейс LH900"
```

---

## Система метрик

Бенчмарк использует систему метрик для всесторонней оценки способностей моделей к tool calling.

### Основные метрики

#### Decision — Проверка факта вызова инструментов

**Что оценивает:** Принял ли агент решение использовать tools.

**Значения:** `1/0` (бинарная метрика)
- `1` — модель вызвала инструмент
- `0` — модель не вызвала инструмент

---

#### Tool Selection — Сопоставление ожидаемых и вызванных инструментов

**Что оценивает:** Правильность выбора конкретного инструмента из доступных.

**Значения:** `от 0 до 1`

---

#### Param — Сопоставление ожидаемых и фактически использованных параметров

**Что оценивает:** Корректность передачи параметров в инструменты.

**Значения:** `от 0 до 1` (непрерывная метрика)

---

#### Result — Комплексная метрика

**Что оценивает:** Общий результат выполнения, включающий оценку как Tool Selection, так и Param.

**Значения:** `от 0 до 1`

---

#### Adaptability — Адаптация к изменениям в запросе

**Что оценивает:** Способность модели вызвать только последний инструмент из тех, что запрашиваются в изменяющемся запросе.

**Значения:** `1/0` (бинарная метрика)

---

#### Ambiguity — Оценка работы с неоднозначными запросами

**Что оценивает:** Работу с запросами, содержащими опечатки, или с неоднозначными запросами, в которых не хватает параметров для выполнения.

**Значения:** `1/0.5/0`

**Сценарии оценки:**

**Сценарий 1: `requires_clarification = True`**
- `1` — модель **не вызвала** инструмент И **не передала** параметры (запросила уточнение)
- `0` — модель вызвала хоть что-то (неверное поведение)

**Сценарий 2: `requires_clarification = False`**
- `1` — точно совпало (tool + params)
- `0.5` — совпал только tool или только params
- `0` — ни tool, ни params не совпали

---

#### Noise — Игнорирование шума

**Что оценивает:** Способность модели игнорировать шум в запросе: не обращать на него внимания, не включать его в output и не вызывать лишних инструментов.

**Значения:** `1/0` (бинарная метрика)

---

#### Error Handling — Обработка ошибочных ситуаций

**Что оценивает:** Способность агента не вызывать инструменты, если запрашивается несуществующий инструмент.

**Значения:** `1/0` (бинарная метрика)

---

#### Execution — Соблюдение строгого порядка вызова инструментов

**Что оценивает:** Способность агента выполнять последовательные операции в правильном порядке.

**Значения:** `1/0` (бинарная метрика)

---

### Формула итоговой оценки

Итоговая оценка рассчитывается по-разному в зависимости от количества применимых метрик для каждого запроса:

#### Для запросов с 4 базовыми метриками

```
Score = 0.30×Decision + 0.30×ToolSelection + 0.22×Param + 0.18×Result
```

**Применяется к:** Явные и неявные запросы без дополнительных проверок

#### Для запросов с 5 метриками (4 базовые + 1 специализированная)

```
Score = 0.28×Decision + 0.28×ToolSelection + 0.20×Param + 0.04×Result + 0.20×SpecificMetric
```

где `SpecificMetric` — одна из: Ambiguity, Noise, Adaptability, ErrorHandling, Execution

**Применяется к:** Запросы, проверяющие специфические навыки

#### Финальная оценка модели

```python
FinalScore = (Σ ScorePerQuery / TotalQueries) × 100
```

**Пример расчёта:**

Модель прошла 360 запросов:
- 270 запросов с 4 метриками (средний score: 0.65)
- 90 запросов с 5 метриками (средний score: 0.55)

```
FinalScore = ((270×0.65 + 90×0.55) / 360) × 100 = 62.5
```

### Интерпретация результатов

| Диапазон | Уровень | Интерпретация |
|----------|---------|---------------|
| 90-100 | Отличный | Модель демонстрирует высокую точность во всех аспектах tool calling |
| 70-89 | Хороший | Модель надёжно работает с большинством сценариев |
| 50-69 | Средний | Модель справляется с базовыми задачами, но имеет проблемы в сложных сценариях |
| 30-49 | Низкий | Модель испытывает значительные трудности с tool calling |
| 0-29 | Критический | Модель не способна корректно работать с инструментами |

---

## Лидерборд

Актуальные результаты всех протестированных моделей доступны на публичном лидерборде:
https://de.kotyan.com